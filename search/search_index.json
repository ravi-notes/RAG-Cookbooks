{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RAG Complete Cookbook Welcome to the Retrieval-Augmented Generation (RAG) documentation set. This site contains 8 self\u2011contained Markdown cookbooks: High\u2011level overview Deep theory Core components Pipeline walkthrough RAG variants Limitations & debugging Colab prerequisites Final summary & checklist Use the left navigation to browse each stage.","title":"Home"},{"location":"#rag-complete-cookbook","text":"Welcome to the Retrieval-Augmented Generation (RAG) documentation set. This site contains 8 self\u2011contained Markdown cookbooks: High\u2011level overview Deep theory Core components Pipeline walkthrough RAG variants Limitations & debugging Colab prerequisites Final summary & checklist Use the left navigation to browse each stage.","title":"RAG Complete Cookbook"},{"location":"stage1_overview/","text":"RAG = \u201cLLM that first looks things up, then answers using what it found,\u201d instead of answering only from its frozen training. Stage 1 \u2013 High-Level Conceptual Overview 1. What problem does RAG solve? LLMs alone: Can hallucinate (sound confident but be wrong) Don\u2019t know new/private data (trained on past public data) Can\u2019t \u201cremember\u201d huge document sets in one prompt RAG solves: \u201cHow do I make an LLM answer using my data (docs, DBs, PDFs, wiki, logs, etc.), while: reducing hallucinations keeping answers grounded in real snippets updating knowledge without retraining the model?\u201d 2. One-sentence definition Retrieval-Augmented Generation (RAG) = A pattern where an LLM retrieves relevant pieces of external data first, then generates an answer using both the user\u2019s question and the retrieved data. 3. Intuition (no math, just idea) Think of RAG as: A smart student taking an exam: Vanilla LLM = student answering from memory only RAG = student allowed to quickly search notes/books, then answer Key intuition: Separate: \u201cStore and search knowledge\u201d (retrieval system) \u201cWrite the answer\u201d (LLM) Then chain them: Question \u2192 Search notes \u2192 Feed notes + question to LLM \u2192 Answer. 4. Key moving parts (bird\u2019s-eye view) At high level, a RAG system has: Knowledge store Your documents broken into small chunks and stored so they can be searched quickly. Retriever Given a question, it finds the most relevant chunks from the knowledge store. Generator (LLM) Takes: the original question the retrieved chunks and writes a coherent answer grounded in those chunks. Later we\u2019ll name them more precisely (embeddings, vector DB, etc.), but conceptually it\u2019s just: Knowledge base + Search + Writer 5. End-to-end flow (very high level) User asks: \u201cWhat are the side effects of drug X mentioned in our internal clinical guidelines?\u201d System searches: It looks through your stored docs and picks a few relevant passages. LLM answers: It reads the question + those passages and writes an answer that: cites or uses that content ideally avoids inventing facts not in the docs. Process: Question \u2192 Retrieve relevant info \u2192 Generate answer using that info This is the \u201cR \u2192 G\u201d in RAG. 6. Simple ASCII diagram \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 User Question \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RETRIEVER \u2502 \u2502 (search relevant \u2502 \u2502 chunks/docs) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Retrieved Chunks \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Question + Chunks) v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 GENERATOR (LLM) \u2502 \u2502 writes answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Final Answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And in the background, you have: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Document Collection \u2502 \u2502 (PDFs, pages, tickets,\u2026) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v [Preprocessed & indexed into] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Knowledge Store (Index) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The retriever searches this knowledge store. 7. Where RAG fits in modern LLM systems RAG is commonly used for: Chat over docs \u201cAsk questions about these PDFs / Confluence / Notion / codebase.\u201d Enterprise Q&A Internal policies, HR docs, product manuals. Customer support bots Use ticket history + FAQ + documentation. Search + answer experience \u201cAI search\u201d that shows sources and summary together. Conceptual stack: User \u2194 Chat UI \u2193 RAG Layer (retrieve + prompt) \u2193 Base LLM \u2193 Answer RAG is a middleware pattern between your data and the LLM. 8. Why this is a big deal You can: Plug an LLM into any private corpus (company docs, wiki, DB dumps) Update knowledge by updating the index, without retraining the LLM Get more trustworthy answers by asking the LLM to \u201cstick to retrieved context\u201d RAG is therefore the default pattern for \u201cLLM + your data\u201d systems.s","title":"1 High\u2011level overview"},{"location":"stage1_overview/#stage-1-high-level-conceptual-overview","text":"","title":"Stage 1 \u2013 High-Level Conceptual Overview"},{"location":"stage1_overview/#1-what-problem-does-rag-solve","text":"LLMs alone: Can hallucinate (sound confident but be wrong) Don\u2019t know new/private data (trained on past public data) Can\u2019t \u201cremember\u201d huge document sets in one prompt RAG solves: \u201cHow do I make an LLM answer using my data (docs, DBs, PDFs, wiki, logs, etc.), while: reducing hallucinations keeping answers grounded in real snippets updating knowledge without retraining the model?\u201d","title":"1. What problem does RAG solve?"},{"location":"stage1_overview/#2-one-sentence-definition","text":"Retrieval-Augmented Generation (RAG) = A pattern where an LLM retrieves relevant pieces of external data first, then generates an answer using both the user\u2019s question and the retrieved data.","title":"2. One-sentence definition"},{"location":"stage1_overview/#3-intuition-no-math-just-idea","text":"Think of RAG as: A smart student taking an exam: Vanilla LLM = student answering from memory only RAG = student allowed to quickly search notes/books, then answer Key intuition: Separate: \u201cStore and search knowledge\u201d (retrieval system) \u201cWrite the answer\u201d (LLM) Then chain them: Question \u2192 Search notes \u2192 Feed notes + question to LLM \u2192 Answer.","title":"3. Intuition (no math, just idea)"},{"location":"stage1_overview/#4-key-moving-parts-birds-eye-view","text":"At high level, a RAG system has: Knowledge store Your documents broken into small chunks and stored so they can be searched quickly. Retriever Given a question, it finds the most relevant chunks from the knowledge store. Generator (LLM) Takes: the original question the retrieved chunks and writes a coherent answer grounded in those chunks. Later we\u2019ll name them more precisely (embeddings, vector DB, etc.), but conceptually it\u2019s just: Knowledge base + Search + Writer","title":"4. Key moving parts (bird\u2019s-eye view)"},{"location":"stage1_overview/#5-end-to-end-flow-very-high-level","text":"User asks: \u201cWhat are the side effects of drug X mentioned in our internal clinical guidelines?\u201d System searches: It looks through your stored docs and picks a few relevant passages. LLM answers: It reads the question + those passages and writes an answer that: cites or uses that content ideally avoids inventing facts not in the docs. Process: Question \u2192 Retrieve relevant info \u2192 Generate answer using that info This is the \u201cR \u2192 G\u201d in RAG.","title":"5. End-to-end flow (very high level)"},{"location":"stage1_overview/#6-simple-ascii-diagram","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 User Question \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 RETRIEVER \u2502 \u2502 (search relevant \u2502 \u2502 chunks/docs) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Retrieved Chunks \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Question + Chunks) v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 GENERATOR (LLM) \u2502 \u2502 writes answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Final Answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 And in the background, you have: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Document Collection \u2502 \u2502 (PDFs, pages, tickets,\u2026) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 v [Preprocessed & indexed into] \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Knowledge Store (Index) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The retriever searches this knowledge store.","title":"6. Simple ASCII diagram"},{"location":"stage1_overview/#7-where-rag-fits-in-modern-llm-systems","text":"RAG is commonly used for: Chat over docs \u201cAsk questions about these PDFs / Confluence / Notion / codebase.\u201d Enterprise Q&A Internal policies, HR docs, product manuals. Customer support bots Use ticket history + FAQ + documentation. Search + answer experience \u201cAI search\u201d that shows sources and summary together. Conceptual stack: User \u2194 Chat UI \u2193 RAG Layer (retrieve + prompt) \u2193 Base LLM \u2193 Answer RAG is a middleware pattern between your data and the LLM.","title":"7. Where RAG fits in modern LLM systems"},{"location":"stage1_overview/#8-why-this-is-a-big-deal","text":"You can: Plug an LLM into any private corpus (company docs, wiki, DB dumps) Update knowledge by updating the index, without retraining the LLM Get more trustworthy answers by asking the LLM to \u201cstick to retrieved context\u201d RAG is therefore the default pattern for \u201cLLM + your data\u201d systems.s","title":"8. Why this is a big deal"},{"location":"stage2_deep_theory/","text":"Stage 2 \u2013 Deep Theory (First-Principles Explanation) 1. Why LLMs need external retrieval LLMs = probabilistic next-token predictors. Fundamentals: They do not store facts explicitly ; they store statistical patterns. Their \u201cknowledge\u201d is: approximate compressed static (frozen after training) They cannot: add new knowledge without retraining guarantee factual recall remember large custom corpora on demand Conclusion: LLMs alone cannot guarantee correctness or freshness. RAG adds a deterministic information source to fix this. 2. Formal RAG idea (conceptual math) Break the answer generation into two conditional steps: Retrieval step: Select a set of documents (D^ ) from a corpus (C) that are relevant to a query (q). Conceptually: (D^ = \\text{Retrieve}(q, C)) Generation step: Use an LLM to produce an answer conditioned on both the query and retrieved documents. Conceptually: (A = \\text{LLM}(q, D^*)) This separates: Knowledge selection (deterministic, index-based) Natural language reasoning (probabilistic, model-based) This two-stage decomposition is core to RAG. 3. Embeddings as the retrieval backbone Intuition: Convert text into a vector (dense representation) Semantic similarity \u2248 geometric closeness Allows \u201cmeaning-based\u201d search instead of keyword search Properties: Embedding models produce vectors in 384\u20134096 dimensions Similarity computed via: cosine similarity dot product Euclidean distance (rare in RAG) Core theoretical idea: Semantic meaning \u2192 geometry Text with similar meaning \u2192 vectors close in space. 4. Vector database theory (what makes it special) A vector DB needs to solve two problems: Indexing large numbers of high-dimensional vectors Fast approximate nearest neighbor search (ANN) \u2192 retrieves top-k closest vectors in milliseconds ANN algorithms: HNSW (Hierarchical Navigable Small World) IVF/Flat Product quantization Graph-based search Key theory: Trade perfect accuracy for massive speed gains with negligible correctness loss. 5. Retriever theory A retriever selects relevant chunks from the corpus. Two main approaches: A. Dense retrieval (default in modern RAG) Compare query embedding with chunk embeddings Strength: semantic match Weakness: may miss keyword-specific info (e.g., numbers, rare entities) B. Sparse retrieval (classic search) BM25, TF-IDF Strength: exact term matching, good for numbers, names Weakness: poor semantic understanding RAG often uses hybrid retrieval = dense + sparse. 6. Generator (LLM) theory: How conditioning works LLMs condition on text via attention: Input tokens = question + retrieved chunks Self-attention creates token\u2013token relationships The model grounds its answer on provided chunk tokens (if prompted correctly) Important theoretical limits: Context window determines max number of tokens it can consider More chunks \u2260 always better (attention dilution) 7. Theoretical benefits of RAG Compositionality: Separate \u201cknowledge lookup\u201d from \u201creasoning\u201d Scalable: Add documents without retraining the LLM Controllable: Swap retriever, adjust chunking, tweak index Auditable: Show retrieved sources; explain why an answer was produced Freshness: Index can be updated daily/hourly 8. Theoretical failure modes (root causes) Retrieval failure Wrong or irrelevant chunks retrieved \u2192 Generator hallucinates or guesses Context overload Too many chunks reduce attention fidelity Semantic gap Embedding model misrepresents niche topics Chunking mismatch Chunks too small \u2192 lost context Chunks too large \u2192 irrelevant noisy text LLM drift Model invents details unless forced to use the context RAG theory = \u201cfix retrieval, fix chunking, fix prompt \u2192 fix system.\u201d 9. Where theory meets practice Real RAG = orchestration of: Representation (embeddings) Indexing (vector DB) Routing (retriever) Context construction (prompt) Generation (LLM) Each part has independent theoretical assumptions; breaking one breaks the chain.","title":"2  Deep Theory (First-Principles Explanation)"},{"location":"stage2_deep_theory/#stage-2-deep-theory-first-principles-explanation","text":"","title":"Stage 2 \u2013 Deep Theory (First-Principles Explanation)"},{"location":"stage2_deep_theory/#1-why-llms-need-external-retrieval","text":"LLMs = probabilistic next-token predictors. Fundamentals: They do not store facts explicitly ; they store statistical patterns. Their \u201cknowledge\u201d is: approximate compressed static (frozen after training) They cannot: add new knowledge without retraining guarantee factual recall remember large custom corpora on demand Conclusion: LLMs alone cannot guarantee correctness or freshness. RAG adds a deterministic information source to fix this.","title":"1. Why LLMs need external retrieval"},{"location":"stage2_deep_theory/#2-formal-rag-idea-conceptual-math","text":"Break the answer generation into two conditional steps: Retrieval step: Select a set of documents (D^ ) from a corpus (C) that are relevant to a query (q). Conceptually: (D^ = \\text{Retrieve}(q, C)) Generation step: Use an LLM to produce an answer conditioned on both the query and retrieved documents. Conceptually: (A = \\text{LLM}(q, D^*)) This separates: Knowledge selection (deterministic, index-based) Natural language reasoning (probabilistic, model-based) This two-stage decomposition is core to RAG.","title":"2. Formal RAG idea (conceptual math)"},{"location":"stage2_deep_theory/#3-embeddings-as-the-retrieval-backbone","text":"Intuition: Convert text into a vector (dense representation) Semantic similarity \u2248 geometric closeness Allows \u201cmeaning-based\u201d search instead of keyword search Properties: Embedding models produce vectors in 384\u20134096 dimensions Similarity computed via: cosine similarity dot product Euclidean distance (rare in RAG) Core theoretical idea: Semantic meaning \u2192 geometry Text with similar meaning \u2192 vectors close in space.","title":"3. Embeddings as the retrieval backbone"},{"location":"stage2_deep_theory/#4-vector-database-theory-what-makes-it-special","text":"A vector DB needs to solve two problems: Indexing large numbers of high-dimensional vectors Fast approximate nearest neighbor search (ANN) \u2192 retrieves top-k closest vectors in milliseconds ANN algorithms: HNSW (Hierarchical Navigable Small World) IVF/Flat Product quantization Graph-based search Key theory: Trade perfect accuracy for massive speed gains with negligible correctness loss.","title":"4. Vector database theory (what makes it special)"},{"location":"stage2_deep_theory/#5-retriever-theory","text":"A retriever selects relevant chunks from the corpus. Two main approaches: A. Dense retrieval (default in modern RAG) Compare query embedding with chunk embeddings Strength: semantic match Weakness: may miss keyword-specific info (e.g., numbers, rare entities) B. Sparse retrieval (classic search) BM25, TF-IDF Strength: exact term matching, good for numbers, names Weakness: poor semantic understanding RAG often uses hybrid retrieval = dense + sparse.","title":"5. Retriever theory"},{"location":"stage2_deep_theory/#6-generator-llm-theory-how-conditioning-works","text":"LLMs condition on text via attention: Input tokens = question + retrieved chunks Self-attention creates token\u2013token relationships The model grounds its answer on provided chunk tokens (if prompted correctly) Important theoretical limits: Context window determines max number of tokens it can consider More chunks \u2260 always better (attention dilution)","title":"6. Generator (LLM) theory: How conditioning works"},{"location":"stage2_deep_theory/#7-theoretical-benefits-of-rag","text":"Compositionality: Separate \u201cknowledge lookup\u201d from \u201creasoning\u201d Scalable: Add documents without retraining the LLM Controllable: Swap retriever, adjust chunking, tweak index Auditable: Show retrieved sources; explain why an answer was produced Freshness: Index can be updated daily/hourly","title":"7. Theoretical benefits of RAG"},{"location":"stage2_deep_theory/#8-theoretical-failure-modes-root-causes","text":"Retrieval failure Wrong or irrelevant chunks retrieved \u2192 Generator hallucinates or guesses Context overload Too many chunks reduce attention fidelity Semantic gap Embedding model misrepresents niche topics Chunking mismatch Chunks too small \u2192 lost context Chunks too large \u2192 irrelevant noisy text LLM drift Model invents details unless forced to use the context RAG theory = \u201cfix retrieval, fix chunking, fix prompt \u2192 fix system.\u201d","title":"8. Theoretical failure modes (root causes)"},{"location":"stage2_deep_theory/#9-where-theory-meets-practice","text":"Real RAG = orchestration of: Representation (embeddings) Indexing (vector DB) Routing (retriever) Context construction (prompt) Generation (LLM) Each part has independent theoretical assumptions; breaking one breaks the chain.","title":"9. Where theory meets practice"},{"location":"stage3_components/","text":"Stage 3 \u2013 Components and Their Internals 0. BLUF RAG has four core components: Embeddings \u2192 Vector DB \u2192 Retriever \u2192 Generator . Each has internal mechanics that determine overall quality. 1. Embeddings (Representation Layer) Purpose Convert text into dense numeric vectors that capture meaning. Internals Model type: Sentence Transformers, OpenAI, Cohere, bge Output: A vector (e.g., 768 or 1024 dimensions) Training objective: Contrastive learning (pull similar texts together, push dissimilar apart) Tokenization: Break text into tokens; model pools token-level representations \u2192 a single vector Pooling strategies: Mean pooling (common, stable) CLS token pooling (model-dependent) What embeddings care about Semantic similarity Context around a passage Paraphrases Topic affinity What embeddings fail at Exact numeric matches Very long documents (truncate) Tables, code, formulas (unless specialized) 2. Chunking (Preprocessing Layer) Purpose Split documents into manageable, semantically coherent pieces. Key internals Chunk size: 200\u20131000 tokens depending on domain Chunk overlap: 10\u201325% to preserve context between segments Segmentation strategy: Sentence-based Paragraph-based Markdown/HTML structure Sliding window for technical docs Failure points Too small \u2192 lost narrative Too big \u2192 irrelevant filler reduces retrieval accuracy No overlap \u2192 boundary information lost Chunking = the \u201cgranularity dial\u201d of RAG. 3. Vector Database (Index Layer) Purpose Store embeddings and support fast similarity search. Main components Storage: Vectors + metadata + chunk text Indexing algorithm: HNSW (most used) IVF/PQ (quantized) Flat (exact search, slow) Distance metric: cosine / dot product Filtering: metadata filters (\u201conly from these docs\u201d) How ANN algorithms work (simple) Build a navigable graph where similar vectors are near each other During search, walk the graph to find neighbors quickly Trade small accuracy loss for huge speed gain Popular vector DBs FAISS Chroma Milvus Weaviate Pinecone LanceDB 4. Retriever (Selection Layer) Purpose Given a question \u2192 identify top-K relevant chunks. Types Dense retriever Embed query \u2192 compare with chunk embeddings Pros: semantic, handles rephrasing Cons: may miss names/numbers Sparse retriever BM25, TF-IDF Pros: exact token matching Cons: weak semantics Hybrid retriever Weighted mix of dense + sparse Best of both worlds Rerankers (optional) Cross-encoder rerank Reads (query, chunk) together Much more accurate but slower Usually final ranking step after ANN search What retrievers optimize Recall@K (ensure correct chunk is in the top-K) Relevance score Domain specificity 5. Context Builder (Prompt Construction Layer) Purpose Assemble the final input to the LLM. Steps Merge top-K chunks Add instructions (\u201cuse only retrieved info\u201d) Insert citations Control formatting Optimize for context length Common patterns Stuffing: Put all chunks inside one prompt Map-Reduce: Summaries of each chunk \u2192 reduced summary ReAct / Chain-of-Thought: Additional reasoning steps 6. Generator (LLM Layer) Purpose Produce final answer grounded in retrieved evidence. Internals Self-attention : correlates question tokens \u2194 chunk tokens Reasoning : chain-of-thought (hidden or explicit) Grounding : salience weighting of retrieved tokens Biases : hallucination tendency preference for narrative token-level fluency over factuality Model choices Llama Mistral GPT Gemma Qwen What affects accuracy Prompt instructions Size of model Cleanliness of retrieval Chunking quality Reranking effectiveness 7. Putting components together (conceptual stack) User Query \u2502 \u25bc Embed Query \u2502 \u25bc Retriever \u2502 (ANN search) \u2502 \u25bc Retrieved Chunks \u2502 (build prompt) \u2502 \u25bc LLM \u2502 \u25bc Grounded Answer Each box has independent hyperparameters that change system behavior.","title":"3  Core components"},{"location":"stage3_components/#stage-3-components-and-their-internals","text":"","title":"Stage 3 \u2013 Components and Their Internals"},{"location":"stage3_components/#0-bluf","text":"RAG has four core components: Embeddings \u2192 Vector DB \u2192 Retriever \u2192 Generator . Each has internal mechanics that determine overall quality.","title":"0. BLUF"},{"location":"stage3_components/#1-embeddings-representation-layer","text":"","title":"1. Embeddings (Representation Layer)"},{"location":"stage3_components/#purpose","text":"Convert text into dense numeric vectors that capture meaning.","title":"Purpose"},{"location":"stage3_components/#internals","text":"Model type: Sentence Transformers, OpenAI, Cohere, bge Output: A vector (e.g., 768 or 1024 dimensions) Training objective: Contrastive learning (pull similar texts together, push dissimilar apart) Tokenization: Break text into tokens; model pools token-level representations \u2192 a single vector Pooling strategies: Mean pooling (common, stable) CLS token pooling (model-dependent)","title":"Internals"},{"location":"stage3_components/#what-embeddings-care-about","text":"Semantic similarity Context around a passage Paraphrases Topic affinity","title":"What embeddings care about"},{"location":"stage3_components/#what-embeddings-fail-at","text":"Exact numeric matches Very long documents (truncate) Tables, code, formulas (unless specialized)","title":"What embeddings fail at"},{"location":"stage3_components/#2-chunking-preprocessing-layer","text":"","title":"2. Chunking (Preprocessing Layer)"},{"location":"stage3_components/#purpose_1","text":"Split documents into manageable, semantically coherent pieces.","title":"Purpose"},{"location":"stage3_components/#key-internals","text":"Chunk size: 200\u20131000 tokens depending on domain Chunk overlap: 10\u201325% to preserve context between segments Segmentation strategy: Sentence-based Paragraph-based Markdown/HTML structure Sliding window for technical docs","title":"Key internals"},{"location":"stage3_components/#failure-points","text":"Too small \u2192 lost narrative Too big \u2192 irrelevant filler reduces retrieval accuracy No overlap \u2192 boundary information lost Chunking = the \u201cgranularity dial\u201d of RAG.","title":"Failure points"},{"location":"stage3_components/#3-vector-database-index-layer","text":"","title":"3. Vector Database (Index Layer)"},{"location":"stage3_components/#purpose_2","text":"Store embeddings and support fast similarity search.","title":"Purpose"},{"location":"stage3_components/#main-components","text":"Storage: Vectors + metadata + chunk text Indexing algorithm: HNSW (most used) IVF/PQ (quantized) Flat (exact search, slow) Distance metric: cosine / dot product Filtering: metadata filters (\u201conly from these docs\u201d)","title":"Main components"},{"location":"stage3_components/#how-ann-algorithms-work-simple","text":"Build a navigable graph where similar vectors are near each other During search, walk the graph to find neighbors quickly Trade small accuracy loss for huge speed gain","title":"How ANN algorithms work (simple)"},{"location":"stage3_components/#popular-vector-dbs","text":"FAISS Chroma Milvus Weaviate Pinecone LanceDB","title":"Popular vector DBs"},{"location":"stage3_components/#4-retriever-selection-layer","text":"","title":"4. Retriever (Selection Layer)"},{"location":"stage3_components/#purpose_3","text":"Given a question \u2192 identify top-K relevant chunks.","title":"Purpose"},{"location":"stage3_components/#types","text":"Dense retriever Embed query \u2192 compare with chunk embeddings Pros: semantic, handles rephrasing Cons: may miss names/numbers Sparse retriever BM25, TF-IDF Pros: exact token matching Cons: weak semantics Hybrid retriever Weighted mix of dense + sparse Best of both worlds Rerankers (optional) Cross-encoder rerank Reads (query, chunk) together Much more accurate but slower Usually final ranking step after ANN search","title":"Types"},{"location":"stage3_components/#what-retrievers-optimize","text":"Recall@K (ensure correct chunk is in the top-K) Relevance score Domain specificity","title":"What retrievers optimize"},{"location":"stage3_components/#5-context-builder-prompt-construction-layer","text":"","title":"5. Context Builder (Prompt Construction Layer)"},{"location":"stage3_components/#purpose_4","text":"Assemble the final input to the LLM.","title":"Purpose"},{"location":"stage3_components/#steps","text":"Merge top-K chunks Add instructions (\u201cuse only retrieved info\u201d) Insert citations Control formatting Optimize for context length","title":"Steps"},{"location":"stage3_components/#common-patterns","text":"Stuffing: Put all chunks inside one prompt Map-Reduce: Summaries of each chunk \u2192 reduced summary ReAct / Chain-of-Thought: Additional reasoning steps","title":"Common patterns"},{"location":"stage3_components/#6-generator-llm-layer","text":"","title":"6. Generator (LLM Layer)"},{"location":"stage3_components/#purpose_5","text":"Produce final answer grounded in retrieved evidence.","title":"Purpose"},{"location":"stage3_components/#internals_1","text":"Self-attention : correlates question tokens \u2194 chunk tokens Reasoning : chain-of-thought (hidden or explicit) Grounding : salience weighting of retrieved tokens Biases : hallucination tendency preference for narrative token-level fluency over factuality","title":"Internals"},{"location":"stage3_components/#model-choices","text":"Llama Mistral GPT Gemma Qwen","title":"Model choices"},{"location":"stage3_components/#what-affects-accuracy","text":"Prompt instructions Size of model Cleanliness of retrieval Chunking quality Reranking effectiveness","title":"What affects accuracy"},{"location":"stage3_components/#7-putting-components-together-conceptual-stack","text":"User Query \u2502 \u25bc Embed Query \u2502 \u25bc Retriever \u2502 (ANN search) \u2502 \u25bc Retrieved Chunks \u2502 (build prompt) \u2502 \u25bc LLM \u2502 \u25bc Grounded Answer Each box has independent hyperparameters that change system behavior.","title":"7. Putting components together (conceptual stack)"},{"location":"stage4_pipeline/","text":"Stage 4 \u2013 Pipeline Walkthrough With Diagrams 0. BLUF End-to-end RAG = Ingest \u2192 Index \u2192 Retrieve \u2192 Prepare Context \u2192 Generate Answer . Below is the full pipeline with ASCII diagrams and internal data flow. 1. Complete RAG Pipeline (Bird\u2019s-Eye) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 INGEST PHASE \u2502 \u2502 (Prepare and store your knowledge base) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 1. Chunking \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 2. Embeddings \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 3. Vector Index (DB) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 QUERY PHASE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 4. Query Embedding \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 5. Retriever (ANN + BM25)\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 6. Context Builder \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 7. LLM Generator \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8. Final Answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 2. Ingest Phase (Offline) Purpose Prepare your corpus so the system can retrieve from it at runtime. Step 1. Chunking Original Documents \u2502 \u25bc [Split into chunks of ~300\u2013800 tokens with overlap] \u2502 \u25bc Chunked Corpus Chunking rules: Maintain semantic coherence Include overlaps so meaning doesn\u2019t break Preserve structure (Markdown headings, paragraphs) Step 2. Embeddings Each chunk \u2500\u2500\u25ba Embedding Model \u2500\u2500\u25ba Chunk Vector Output = vector + metadata (doc_id, page_no, text). Step 3. Vector Indexing Chunk Vectors \u2500\u2510 \u25bc [Vector Database] \u2502 \u2514\u2500\u2500 stores: - vectors - raw text - metadata - ANN index Vector DB builds internal ANN graphs for fast retrieval. 3. Query Phase (Online) This is the real-time flow when a user asks a question. Step 4. User Query \u2192 Embedding User Query \"What is X?\" \u2502 \u25bc Embedding Model \u2502 \u25bc Query Vector Step 5. Retrieval Query Vector \u2502 \u25bc ANN Search in Vector DB \u2502 \u25bc Top-K Candidate Chunks \u2502 \u25bc (Optional) Reranker (cross-encoder) \u2502 \u25bc Final Ranked Chunks Hybrid RAG = Dense ANN output + BM25 sparse output \u2192 merged \u2192 reranked. Step 6. Context Builder Final Chunks \u2502 \u25bc [Insert into prompt template] \u2502 \u25bc LLM-ready input: \"You must answer using ONLY the passages below. Question: ... Context: [chunk 1] [chunk 2] [chunk 3]\" Controls: Token limit Deduplication Formatting Citations Instruction tuning Step 7. LLM Generator (Question + Context) \u2502 \u25bc LLM \u2502 \u25bc Answer LLM uses attention to align question tokens with relevant chunk tokens. Step 8. Final Answer Grounded, cite-able output based on your data. 4. Compact ASCII Diagram (Side-by-Side View) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 User Query \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25bc Embed Query \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Retriever \u2502 \u2502 (ANN / Hybrid) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25bc Retrieved Chunks \u2502 \u25bc Build Prompt (Context) \u2502 \u25bc LLM (G) \u2502 \u25bc Final Answer Behind the scenes: Documents \u2192 Chunking \u2192 Embeddings \u2192 Vector DB 5. Data Flow Summary Table Stage Input Processing Output Chunking Raw docs segment text chunks Embedding chunks vector encoding chunk vectors Indexing chunk vectors ANN indexing searchable index Query Embedding query vector encoding query vector Retriever query vector similarity search top-k chunks Context Builder chunks prompt assembly LLM prompt Generator prompt reasoning + synthesis answer","title":"4 Pipeline walkthrough"},{"location":"stage4_pipeline/#stage-4-pipeline-walkthrough-with-diagrams","text":"","title":"Stage 4 \u2013 Pipeline Walkthrough With Diagrams"},{"location":"stage4_pipeline/#0-bluf","text":"End-to-end RAG = Ingest \u2192 Index \u2192 Retrieve \u2192 Prepare Context \u2192 Generate Answer . Below is the full pipeline with ASCII diagrams and internal data flow.","title":"0. BLUF"},{"location":"stage4_pipeline/#1-complete-rag-pipeline-birds-eye","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 INGEST PHASE \u2502 \u2502 (Prepare and store your knowledge base) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 1. Chunking \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 2. Embeddings \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 3. Vector Index (DB) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 QUERY PHASE \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 4. Query Embedding \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 5. Retriever (ANN + BM25)\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 6. Context Builder \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 7. LLM Generator \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 8. Final Answer \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"1. Complete RAG Pipeline (Bird\u2019s-Eye)"},{"location":"stage4_pipeline/#2-ingest-phase-offline","text":"","title":"2. Ingest Phase (Offline)"},{"location":"stage4_pipeline/#purpose","text":"Prepare your corpus so the system can retrieve from it at runtime.","title":"Purpose"},{"location":"stage4_pipeline/#step-1-chunking","text":"Original Documents \u2502 \u25bc [Split into chunks of ~300\u2013800 tokens with overlap] \u2502 \u25bc Chunked Corpus Chunking rules: Maintain semantic coherence Include overlaps so meaning doesn\u2019t break Preserve structure (Markdown headings, paragraphs)","title":"Step 1. Chunking"},{"location":"stage4_pipeline/#step-2-embeddings","text":"Each chunk \u2500\u2500\u25ba Embedding Model \u2500\u2500\u25ba Chunk Vector Output = vector + metadata (doc_id, page_no, text).","title":"Step 2. Embeddings"},{"location":"stage4_pipeline/#step-3-vector-indexing","text":"Chunk Vectors \u2500\u2510 \u25bc [Vector Database] \u2502 \u2514\u2500\u2500 stores: - vectors - raw text - metadata - ANN index Vector DB builds internal ANN graphs for fast retrieval.","title":"Step 3. Vector Indexing"},{"location":"stage4_pipeline/#3-query-phase-online","text":"This is the real-time flow when a user asks a question.","title":"3. Query Phase (Online)"},{"location":"stage4_pipeline/#step-4-user-query-embedding","text":"User Query \"What is X?\" \u2502 \u25bc Embedding Model \u2502 \u25bc Query Vector","title":"Step 4. User Query \u2192 Embedding"},{"location":"stage4_pipeline/#step-5-retrieval","text":"Query Vector \u2502 \u25bc ANN Search in Vector DB \u2502 \u25bc Top-K Candidate Chunks \u2502 \u25bc (Optional) Reranker (cross-encoder) \u2502 \u25bc Final Ranked Chunks Hybrid RAG = Dense ANN output + BM25 sparse output \u2192 merged \u2192 reranked.","title":"Step 5. Retrieval"},{"location":"stage4_pipeline/#step-6-context-builder","text":"Final Chunks \u2502 \u25bc [Insert into prompt template] \u2502 \u25bc LLM-ready input: \"You must answer using ONLY the passages below. Question: ... Context: [chunk 1] [chunk 2] [chunk 3]\" Controls: Token limit Deduplication Formatting Citations Instruction tuning","title":"Step 6. Context Builder"},{"location":"stage4_pipeline/#step-7-llm-generator","text":"(Question + Context) \u2502 \u25bc LLM \u2502 \u25bc Answer LLM uses attention to align question tokens with relevant chunk tokens.","title":"Step 7. LLM Generator"},{"location":"stage4_pipeline/#step-8-final-answer","text":"Grounded, cite-able output based on your data.","title":"Step 8. Final Answer"},{"location":"stage4_pipeline/#4-compact-ascii-diagram-side-by-side-view","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 User Query \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25bc Embed Query \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Retriever \u2502 \u2502 (ANN / Hybrid) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u25bc Retrieved Chunks \u2502 \u25bc Build Prompt (Context) \u2502 \u25bc LLM (G) \u2502 \u25bc Final Answer Behind the scenes: Documents \u2192 Chunking \u2192 Embeddings \u2192 Vector DB","title":"4. Compact ASCII Diagram (Side-by-Side View)"},{"location":"stage4_pipeline/#5-data-flow-summary-table","text":"Stage Input Processing Output Chunking Raw docs segment text chunks Embedding chunks vector encoding chunk vectors Indexing chunk vectors ANN indexing searchable index Query Embedding query vector encoding query vector Retriever query vector similarity search top-k chunks Context Builder chunks prompt assembly LLM prompt Generator prompt reasoning + synthesis answer","title":"5. Data Flow Summary Table"},{"location":"stage5_variants/","text":"Stage 5 \u2013 RAG Variants and Their Tradeoffs 0. BLUF Variants differ in how they retrieve , route , and reason . They exist to fix typical failure modes (poor retrieval, multi-hop reasoning, context overload, niche domains). 1. Basic RAG (Single-Hop) Definition Retrieve top-K chunks once \u2192 pass to LLM \u2192 generate answer. Diagram Query \u2192 Retrieve \u2192 LLM \u2192 Answer Strengths Simple Fast Good for short, direct Q&A Easy to implement (default in most libraries) Weaknesses Fails when answer requires multiple dependent facts Retrieval errors propagate directly to the LLM Limited to top-K context window constraints 2. Multi-Hop RAG Definition LLM performs iterative retrieval : Use the first retrieved context to refine the next query. Diagram Query \u2192 Retrieve \u2192 LLM \u2192 Subquery \u2191 \u2502 \u2514\u2500 Retrieve Again \u2500\u2192 (repeat) Use Cases Questions requiring multi-step reasoning Linking information across documents \u201cExplain relationships between A and B using corpus C\u201d Strengths Much higher recall for complex tasks Better for research, legal, medical, financial analyses Weaknesses More latency Harder to control Requires strong query rewriting skills 3. HyBRID RAG (Dense + Sparse) Definition Combine semantic search (dense) with keyword search (BM25). Diagram Dense ANN \u2500\u2500\u2510 \u251c\u2500\u2500 Merge \u2192 Rerank \u2192 Top-K BM25 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Strengths Best retrieval accuracy for most domains Handles: synonyms exact keywords numeric strings rare entities Reduces embedding model blind spots Weaknesses More components Heavier infra Requires scoring and weighting 4. RAG with Reranking (Cross-Encoder) Definition Retriever collects many candidates (k=50\u2013200) \u2192 cross-encoder re-scores relevance. Diagram Top-100 candidates from ANN \u2193 Cross-Encoder Rerank \u2193 Final Top-K (3\u20138) Strengths Dramatic improvement in precision Eliminates \u201cgarbage context\u201d Especially helpful in: legal biomedical finance code search Weaknesses High latency Higher cost Needs GPU for speed 5. Graph RAG Definition Build a graph of entities/relationships from corpus \u2192 retrieve subgraphs instead of raw chunks. Diagram Corpus \u2192 entity extraction \u2192 graph Query \u2192 graph search \u2192 retrieve subgraph \u2192 LLM Strengths Great for relationship-heavy corpora Multi-hop reasoning becomes explicit Excellent for highly structured domains Weaknesses Heavy preprocessing Requires reliable entity extraction Complex to maintain 6. Agentic / ReAct RAG Definition LLM acts as an agent that can issue actions: Search again Reformulate question Read more chunks Validate evidence Diagram Query \u2193 LLM \u2192 (action: search) \u2192 retriever \u2192 new context \u2193 LLM \u2192 answer Strengths Highly flexible LLM decides when/what to retrieve Long chains of reasoning possible Weaknesses Unpredictable behavior Harder to guarantee privacy constraints Latency spikes 7. Context-Compression RAG Definition Compress chunks before adding to prompt: Summaries Extractive reduction Key sentences only Strengths Fits more info into limited context windows Useful for small LLMs Weaknesses Compression must preserve meaning Summaries may lose vital details 8. Adaptive RAG (Router-Based) Definition Different query types \u2192 different retrievers or knowledge sources. Diagram Router: factual \u2192 RAG reasoning \u2192 chain-of-thought code \u2192 code-index Strengths Reduces irrelevant retrieval Improves accuracy and latency Useful in enterprise setups Weaknesses Requires classifier or heuristics Complex pipeline 9. Comparison Table (Condensed) Variant Purpose Strengths Weaknesses Basic Simple QA Fast, easy Fails on multi-hop Multi-hop Complex reasoning Higher recall Slow, hard Hybrid Better accuracy Recovers dense+BM25 Requires merging logic Reranker Precision Best top-K relevance High latency Graph Relationship-heavy data Structured, multi-hop Heavy preprocessing Agentic Adaptive retrieval Max flexibility Unpredictable Compression Fit more info Good for small LLMs Risk of info loss Adaptive Smart routing Lower noise Complex design","title":"5 RAG Variants"},{"location":"stage5_variants/#stage-5-rag-variants-and-their-tradeoffs","text":"","title":"Stage 5 \u2013 RAG Variants and Their Tradeoffs"},{"location":"stage5_variants/#0-bluf","text":"Variants differ in how they retrieve , route , and reason . They exist to fix typical failure modes (poor retrieval, multi-hop reasoning, context overload, niche domains).","title":"0. BLUF"},{"location":"stage5_variants/#1-basic-rag-single-hop","text":"","title":"1. Basic RAG (Single-Hop)"},{"location":"stage5_variants/#definition","text":"Retrieve top-K chunks once \u2192 pass to LLM \u2192 generate answer.","title":"Definition"},{"location":"stage5_variants/#diagram","text":"Query \u2192 Retrieve \u2192 LLM \u2192 Answer","title":"Diagram"},{"location":"stage5_variants/#strengths","text":"Simple Fast Good for short, direct Q&A Easy to implement (default in most libraries)","title":"Strengths"},{"location":"stage5_variants/#weaknesses","text":"Fails when answer requires multiple dependent facts Retrieval errors propagate directly to the LLM Limited to top-K context window constraints","title":"Weaknesses"},{"location":"stage5_variants/#2-multi-hop-rag","text":"","title":"2. Multi-Hop RAG"},{"location":"stage5_variants/#definition_1","text":"LLM performs iterative retrieval : Use the first retrieved context to refine the next query.","title":"Definition"},{"location":"stage5_variants/#diagram_1","text":"Query \u2192 Retrieve \u2192 LLM \u2192 Subquery \u2191 \u2502 \u2514\u2500 Retrieve Again \u2500\u2192 (repeat)","title":"Diagram"},{"location":"stage5_variants/#use-cases","text":"Questions requiring multi-step reasoning Linking information across documents \u201cExplain relationships between A and B using corpus C\u201d","title":"Use Cases"},{"location":"stage5_variants/#strengths_1","text":"Much higher recall for complex tasks Better for research, legal, medical, financial analyses","title":"Strengths"},{"location":"stage5_variants/#weaknesses_1","text":"More latency Harder to control Requires strong query rewriting skills","title":"Weaknesses"},{"location":"stage5_variants/#3-hybrid-rag-dense-sparse","text":"","title":"3. HyBRID RAG (Dense + Sparse)"},{"location":"stage5_variants/#definition_2","text":"Combine semantic search (dense) with keyword search (BM25).","title":"Definition"},{"location":"stage5_variants/#diagram_2","text":"Dense ANN \u2500\u2500\u2510 \u251c\u2500\u2500 Merge \u2192 Rerank \u2192 Top-K BM25 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Diagram"},{"location":"stage5_variants/#strengths_2","text":"Best retrieval accuracy for most domains Handles: synonyms exact keywords numeric strings rare entities Reduces embedding model blind spots","title":"Strengths"},{"location":"stage5_variants/#weaknesses_2","text":"More components Heavier infra Requires scoring and weighting","title":"Weaknesses"},{"location":"stage5_variants/#4-rag-with-reranking-cross-encoder","text":"","title":"4. RAG with Reranking (Cross-Encoder)"},{"location":"stage5_variants/#definition_3","text":"Retriever collects many candidates (k=50\u2013200) \u2192 cross-encoder re-scores relevance.","title":"Definition"},{"location":"stage5_variants/#diagram_3","text":"Top-100 candidates from ANN \u2193 Cross-Encoder Rerank \u2193 Final Top-K (3\u20138)","title":"Diagram"},{"location":"stage5_variants/#strengths_3","text":"Dramatic improvement in precision Eliminates \u201cgarbage context\u201d Especially helpful in: legal biomedical finance code search","title":"Strengths"},{"location":"stage5_variants/#weaknesses_3","text":"High latency Higher cost Needs GPU for speed","title":"Weaknesses"},{"location":"stage5_variants/#5-graph-rag","text":"","title":"5. Graph RAG"},{"location":"stage5_variants/#definition_4","text":"Build a graph of entities/relationships from corpus \u2192 retrieve subgraphs instead of raw chunks.","title":"Definition"},{"location":"stage5_variants/#diagram_4","text":"Corpus \u2192 entity extraction \u2192 graph Query \u2192 graph search \u2192 retrieve subgraph \u2192 LLM","title":"Diagram"},{"location":"stage5_variants/#strengths_4","text":"Great for relationship-heavy corpora Multi-hop reasoning becomes explicit Excellent for highly structured domains","title":"Strengths"},{"location":"stage5_variants/#weaknesses_4","text":"Heavy preprocessing Requires reliable entity extraction Complex to maintain","title":"Weaknesses"},{"location":"stage5_variants/#6-agentic-react-rag","text":"","title":"6. Agentic / ReAct RAG"},{"location":"stage5_variants/#definition_5","text":"LLM acts as an agent that can issue actions: Search again Reformulate question Read more chunks Validate evidence","title":"Definition"},{"location":"stage5_variants/#diagram_5","text":"Query \u2193 LLM \u2192 (action: search) \u2192 retriever \u2192 new context \u2193 LLM \u2192 answer","title":"Diagram"},{"location":"stage5_variants/#strengths_5","text":"Highly flexible LLM decides when/what to retrieve Long chains of reasoning possible","title":"Strengths"},{"location":"stage5_variants/#weaknesses_5","text":"Unpredictable behavior Harder to guarantee privacy constraints Latency spikes","title":"Weaknesses"},{"location":"stage5_variants/#7-context-compression-rag","text":"","title":"7. Context-Compression RAG"},{"location":"stage5_variants/#definition_6","text":"Compress chunks before adding to prompt: Summaries Extractive reduction Key sentences only","title":"Definition"},{"location":"stage5_variants/#strengths_6","text":"Fits more info into limited context windows Useful for small LLMs","title":"Strengths"},{"location":"stage5_variants/#weaknesses_6","text":"Compression must preserve meaning Summaries may lose vital details","title":"Weaknesses"},{"location":"stage5_variants/#8-adaptive-rag-router-based","text":"","title":"8. Adaptive RAG (Router-Based)"},{"location":"stage5_variants/#definition_7","text":"Different query types \u2192 different retrievers or knowledge sources.","title":"Definition"},{"location":"stage5_variants/#diagram_6","text":"Router: factual \u2192 RAG reasoning \u2192 chain-of-thought code \u2192 code-index","title":"Diagram"},{"location":"stage5_variants/#strengths_7","text":"Reduces irrelevant retrieval Improves accuracy and latency Useful in enterprise setups","title":"Strengths"},{"location":"stage5_variants/#weaknesses_7","text":"Requires classifier or heuristics Complex pipeline","title":"Weaknesses"},{"location":"stage5_variants/#9-comparison-table-condensed","text":"Variant Purpose Strengths Weaknesses Basic Simple QA Fast, easy Fails on multi-hop Multi-hop Complex reasoning Higher recall Slow, hard Hybrid Better accuracy Recovers dense+BM25 Requires merging logic Reranker Precision Best top-K relevance High latency Graph Relationship-heavy data Structured, multi-hop Heavy preprocessing Agentic Adaptive retrieval Max flexibility Unpredictable Compression Fit more info Good for small LLMs Risk of info loss Adaptive Smart routing Lower noise Complex design","title":"9. Comparison Table (Condensed)"},{"location":"stage6_limitations/","text":"Stage 6 \u2013 Limitations and Debugging Patterns 0. BLUF RAG almost always fails due to retrieval , chunking , prompting , or LLM drift . Fixing RAG = diagnosing which layer broke. 1. Core Limitations (Structural) A. Retrieval quality bottleneck If the correct chunk never reaches the LLM, the answer cannot be correct. Symptoms: Confident hallucinations \u201cThe answer is not in the provided context.\u201d Nonsense or off-topic responses Root causes: Poor embeddings Chunking too small/large ANN index mis-tuned Domain-mismatch between embedding model and corpus B. Context window limits LLMs can only use token-level attention within their window (e.g., 8k, 32k, 128k). Limits: Only a small slice of corpus is visible Adding more chunks \u2260 better answer At scale (100k+ docs), retrieval precision becomes critical C. Hallucination tendency Even with context, LLMs may: invent missing links over-generalize \u201csmooth\u201d contradictions merge unrelated chunks into one narrative Cause: LLM is trained to be fluent , not factual . D. Embedding model mismatch General-purpose embeddings fail for: medical terminology legal norms code math tables finance documents multilingual corpora E. Chunking failures Chunk too small \u2192 no full meaning Chunk too big \u2192 irrelevant background noise Without good chunking, retrieval collapses. F. Long-tail queries LLMs fail when: Query uses rare terms Query is extremely specific Query requires reading entire document sets (RAG can\u2019t load everything) G. Latency/cost issues Rerankers, hybrid retrieval, graph-RAG increase accuracy but also latency and cost. 2. Failure Modes (What You See and Why) Symptom Likely Cause Answer is hallucinated Retrieval failed; chunks irrelevant Answer is generic Prompt not instructing grounding; poor reranking Answer contradicts docs Bad chunk selection; multiple conflicting chunks Answer is incomplete Only partial chunk retrieved; chunk size too small \u201cI cannot find information\u201d Sparse-only retrieval failing on semantics Good chunks retrieved but answer bad LLM not grounded; context overload 3. Debugging Patterns (Practical Workflow) Pattern 1 \u2014 Inspect retrieved chunks Always check the actual top-K retrieved results. If wrong \u2192 fix retrieval, not the LLM. Checklist: Are chunks topically aligned? Any missing keywords? Too much filler text? Wrong document sections? Pattern 2 \u2014 Tune chunking Start with: size = 300\u2013500 tokens overlap = 50\u2013100 tokens Adjust based on domain: Legal/medical : larger, structured chunks Code : small chunks with file structure preserved Wiki/Markdown : chunk by heading sections Pattern 3 \u2014 Improve embeddings Options: Switch to domain-specific embedding models Use larger embedding models Use multilingual embeddings if corpus spans languages Pattern 4 \u2014 Use hybrid retrieval Dense + BM25 fixes: rare tokens names numbers acronyms code identifiers Often gives the biggest jump in accuracy. Pattern 5 \u2014 Add a reranker Cross-encoders dramatically improve precision. Pipeline: ANN top-50 \u2192 reranker \u2192 top-5 \u2192 LLM Pattern 6 \u2014 Prompt the LLM to ground itself Include constraints: \u201cAnswer only from provided context.\u201d \u201cIf answer not present, say 'Not found'.\u201d \u201cCite the exact context passages.\u201d \u201cDo not add external facts.\u201d These reduce hallucination rate. Pattern 7 \u2014 Reduce context clutter Too many chunks dilute attention. Try: top-3 or top-4 instead of top-10 reranking to get cleaner context context compression for long documents Pattern 8 \u2014 Query rewriting LLM or rule-based agent reformulates the user question before retrieval. Example: Original: \u201cWhat did the report say about testing?\u201d Rewrite: \u201cSummarize test procedures from the 2021 QA audit document.\u201d Improves recall. 4. Quick Debug Decision Tree Bad answer? \u2193 Check retrieved chunks: Bad? \u2192 Fix embedding/chunking/retrieval. Good? \u2192 Move on. \u2193 Check prompt grounding: Not strict? \u2192 Strengthen instructions. \u2193 Check chunk quantity: Too many? \u2192 Reduce K. Too few? \u2192 Increase K + rerank. \u2193 Check domain mismatch: Use domain embedding model. \u2193 Still bad? Add hybrid search / reranker / agentic search. 5. Hard Limits (What RAG Cannot Solve) Does not generate new knowledge beyond docs Cannot ensure 100% truth even with perfect retrieval Cannot aggregate massive corpora above context window without compression systems Cannot replace fine-tuning when task requires reasoning style changes Does not understand diagrams, tables, or images unless multimodal embeddings are used Cannot fix contradictory source documents Cannot guarantee privacy if agentic retrieval is uncontrolled","title":"6 Limitations & debugging"},{"location":"stage6_limitations/#stage-6-limitations-and-debugging-patterns","text":"","title":"Stage 6 \u2013 Limitations and Debugging Patterns"},{"location":"stage6_limitations/#0-bluf","text":"RAG almost always fails due to retrieval , chunking , prompting , or LLM drift . Fixing RAG = diagnosing which layer broke.","title":"0. BLUF"},{"location":"stage6_limitations/#1-core-limitations-structural","text":"","title":"1. Core Limitations (Structural)"},{"location":"stage6_limitations/#a-retrieval-quality-bottleneck","text":"If the correct chunk never reaches the LLM, the answer cannot be correct. Symptoms: Confident hallucinations \u201cThe answer is not in the provided context.\u201d Nonsense or off-topic responses Root causes: Poor embeddings Chunking too small/large ANN index mis-tuned Domain-mismatch between embedding model and corpus","title":"A. Retrieval quality bottleneck"},{"location":"stage6_limitations/#b-context-window-limits","text":"LLMs can only use token-level attention within their window (e.g., 8k, 32k, 128k). Limits: Only a small slice of corpus is visible Adding more chunks \u2260 better answer At scale (100k+ docs), retrieval precision becomes critical","title":"B. Context window limits"},{"location":"stage6_limitations/#c-hallucination-tendency","text":"Even with context, LLMs may: invent missing links over-generalize \u201csmooth\u201d contradictions merge unrelated chunks into one narrative Cause: LLM is trained to be fluent , not factual .","title":"C. Hallucination tendency"},{"location":"stage6_limitations/#d-embedding-model-mismatch","text":"General-purpose embeddings fail for: medical terminology legal norms code math tables finance documents multilingual corpora","title":"D. Embedding model mismatch"},{"location":"stage6_limitations/#e-chunking-failures","text":"Chunk too small \u2192 no full meaning Chunk too big \u2192 irrelevant background noise Without good chunking, retrieval collapses.","title":"E. Chunking failures"},{"location":"stage6_limitations/#f-long-tail-queries","text":"LLMs fail when: Query uses rare terms Query is extremely specific Query requires reading entire document sets (RAG can\u2019t load everything)","title":"F. Long-tail queries"},{"location":"stage6_limitations/#g-latencycost-issues","text":"Rerankers, hybrid retrieval, graph-RAG increase accuracy but also latency and cost.","title":"G. Latency/cost issues"},{"location":"stage6_limitations/#2-failure-modes-what-you-see-and-why","text":"Symptom Likely Cause Answer is hallucinated Retrieval failed; chunks irrelevant Answer is generic Prompt not instructing grounding; poor reranking Answer contradicts docs Bad chunk selection; multiple conflicting chunks Answer is incomplete Only partial chunk retrieved; chunk size too small \u201cI cannot find information\u201d Sparse-only retrieval failing on semantics Good chunks retrieved but answer bad LLM not grounded; context overload","title":"2. Failure Modes (What You See and Why)"},{"location":"stage6_limitations/#3-debugging-patterns-practical-workflow","text":"","title":"3. Debugging Patterns (Practical Workflow)"},{"location":"stage6_limitations/#pattern-1-inspect-retrieved-chunks","text":"Always check the actual top-K retrieved results. If wrong \u2192 fix retrieval, not the LLM. Checklist: Are chunks topically aligned? Any missing keywords? Too much filler text? Wrong document sections?","title":"Pattern 1 \u2014 Inspect retrieved chunks"},{"location":"stage6_limitations/#pattern-2-tune-chunking","text":"Start with: size = 300\u2013500 tokens overlap = 50\u2013100 tokens Adjust based on domain: Legal/medical : larger, structured chunks Code : small chunks with file structure preserved Wiki/Markdown : chunk by heading sections","title":"Pattern 2 \u2014 Tune chunking"},{"location":"stage6_limitations/#pattern-3-improve-embeddings","text":"Options: Switch to domain-specific embedding models Use larger embedding models Use multilingual embeddings if corpus spans languages","title":"Pattern 3 \u2014 Improve embeddings"},{"location":"stage6_limitations/#pattern-4-use-hybrid-retrieval","text":"Dense + BM25 fixes: rare tokens names numbers acronyms code identifiers Often gives the biggest jump in accuracy.","title":"Pattern 4 \u2014 Use hybrid retrieval"},{"location":"stage6_limitations/#pattern-5-add-a-reranker","text":"Cross-encoders dramatically improve precision. Pipeline: ANN top-50 \u2192 reranker \u2192 top-5 \u2192 LLM","title":"Pattern 5 \u2014 Add a reranker"},{"location":"stage6_limitations/#pattern-6-prompt-the-llm-to-ground-itself","text":"Include constraints: \u201cAnswer only from provided context.\u201d \u201cIf answer not present, say 'Not found'.\u201d \u201cCite the exact context passages.\u201d \u201cDo not add external facts.\u201d These reduce hallucination rate.","title":"Pattern 6 \u2014 Prompt the LLM to ground itself"},{"location":"stage6_limitations/#pattern-7-reduce-context-clutter","text":"Too many chunks dilute attention. Try: top-3 or top-4 instead of top-10 reranking to get cleaner context context compression for long documents","title":"Pattern 7 \u2014 Reduce context clutter"},{"location":"stage6_limitations/#pattern-8-query-rewriting","text":"LLM or rule-based agent reformulates the user question before retrieval. Example: Original: \u201cWhat did the report say about testing?\u201d Rewrite: \u201cSummarize test procedures from the 2021 QA audit document.\u201d Improves recall.","title":"Pattern 8 \u2014 Query rewriting"},{"location":"stage6_limitations/#4-quick-debug-decision-tree","text":"Bad answer? \u2193 Check retrieved chunks: Bad? \u2192 Fix embedding/chunking/retrieval. Good? \u2192 Move on. \u2193 Check prompt grounding: Not strict? \u2192 Strengthen instructions. \u2193 Check chunk quantity: Too many? \u2192 Reduce K. Too few? \u2192 Increase K + rerank. \u2193 Check domain mismatch: Use domain embedding model. \u2193 Still bad? Add hybrid search / reranker / agentic search.","title":"4. Quick Debug Decision Tree"},{"location":"stage6_limitations/#5-hard-limits-what-rag-cannot-solve","text":"Does not generate new knowledge beyond docs Cannot ensure 100% truth even with perfect retrieval Cannot aggregate massive corpora above context window without compression systems Cannot replace fine-tuning when task requires reasoning style changes Does not understand diagrams, tables, or images unless multimodal embeddings are used Cannot fix contradictory source documents Cannot guarantee privacy if agentic retrieval is uncontrolled","title":"5. Hard Limits (What RAG Cannot Solve)"},{"location":"stage7_colab_prereqs/","text":"Stage 7 \u2013 Minimal Prerequisites for a Colab Prototype BLUF To build the smallest possible working RAG in Google Colab, you only need: a few files, 2) one embedding model, 3) one vector DB, 4) one retriever, 5) one LLM call. Nothing else. 1. Minimal Conceptual Requirements A. One small corpus Examples: A text file A PDF converted to text A folder of .txt notes A few paragraphs you paste manually Keep it tiny to reduce noise. B. A chunker Simplest possible: Fixed size: 300\u2013500 tokens Overlap: 50\u2013100 tokens Use recursive text splitter in LangChain or a simple Python function C. An embedding model Pick one: bge-small-en (fast, free, excellent) sentence-transformers/all-MiniLM-L6-v2 text-embedding-3-small (OpenAI; optional) For Colab: sentence-transformers is easiest. D. A vector store Simplest/local options: FAISS (best for Colab) Chroma (optional) FAISS keeps everything in RAM \u2192 zero infra complexity. E. A retriever FAISSStore.asRetriever(k=3) or Chroma.asRetriever(). F. An LLM At minimal: GPT-4o-mini or GPT-4.1 via API Or a local model like Qwen2.5 0.5B via transformers (only for demonstration) For Colab, simplest working pipeline uses OpenAI API. 2. Minimal Technical Requirements (Software) Install only these: pip install langchain sentence-transformers faiss-cpu openai Optional: PyPDF2 for PDFs tiktoken for token counting 3. Minimum Pipeline (Executable Sequence) Step 1. Load documents Read .txt or extract text from PDF Step 2. Chunk the text Use LangChain\u2019s RecursiveCharacterTextSplitter or your own Step 3. Embed chunks Using Sentence Transformers model Produce vectors Step 4. Build index Store vectors in FAISS Save chunk texts as metadata Step 5. Build retriever FAISS \u2192 retriever Step 6. Build RAG chain Query \u2192 embed \u2192 retrieve top-K chunks \u2192 format prompt \u2192 LLM \u2192 answer Step 7. Test Ask a question and inspect: Retrieved chunks Generated answer 4. Minimal Working RAG Code Skeleton (Pseudocode) This is not full code; it\u2019s the conceptual \u201cshape\u201d of the notebook. # 1. Load text text = open('notes.txt').read() # 2. Chunk chunks = chunk(text, size=500, overlap=100) # 3. Embed embeddings = embed(chunks) # 4. Index faiss_index = FAISS.from_embeddings(embeddings, chunks) # 5. Retriever retriever = faiss_index.as_retriever(k=5) # 6. Query query = \"What does the document say about X?\" query_vector = embed(query) results = retriever.get_relevant_documents(query) # 7. Prompt + LLM answer = call_llm(query, results) print(answer) This is literally all needed for the smallest RAG demo. 5. Minimal Notebook Structure (8\u201310 cells) Install packages Import libraries Load data Chunk data Load embedding model Generate + store embeddings Build FAISS index Query via retriever Build prompt Call LLM 6. Practical Tips for the Beginner Prototype Use very small datasets (2\u20133 paragraphs) so you can inspect every chunk. Print retrieved chunks each time; this is essential for learning retrieval behavior. Start with top_k = 3 . Keep your LLM prompt extremely simple in Stage 1. Do not add rerankers, hybrid search, or agent pipelines initially. Goal: functional correctness, not sophistication . 7. Smallest Buildable RAG Architecture Diagram Documents \u2500\u2192 Chunker \u2500\u2192 Embedder \u2500\u2192 FAISS Index \u2502 Query \u2500\u2192\u2502 \u25bc Retriever \u2502 \u25bc Prompt Constructor \u2502 \u25bc LLM \u25bc Answer Stage 7 complete. Say \u201ccontinue\u201d to proceed to Stage 8 \u2192 Final ultra-short summary + learning checklist .","title":"7 Colab prerequisites"},{"location":"stage7_colab_prereqs/#stage-7-minimal-prerequisites-for-a-colab-prototype","text":"","title":"Stage 7 \u2013 Minimal Prerequisites for a Colab Prototype"},{"location":"stage7_colab_prereqs/#bluf","text":"To build the smallest possible working RAG in Google Colab, you only need: a few files, 2) one embedding model, 3) one vector DB, 4) one retriever, 5) one LLM call. Nothing else.","title":"BLUF"},{"location":"stage7_colab_prereqs/#1-minimal-conceptual-requirements","text":"","title":"1. Minimal Conceptual Requirements"},{"location":"stage7_colab_prereqs/#a-one-small-corpus","text":"Examples: A text file A PDF converted to text A folder of .txt notes A few paragraphs you paste manually Keep it tiny to reduce noise.","title":"A. One small corpus"},{"location":"stage7_colab_prereqs/#b-a-chunker","text":"Simplest possible: Fixed size: 300\u2013500 tokens Overlap: 50\u2013100 tokens Use recursive text splitter in LangChain or a simple Python function","title":"B. A chunker"},{"location":"stage7_colab_prereqs/#c-an-embedding-model","text":"Pick one: bge-small-en (fast, free, excellent) sentence-transformers/all-MiniLM-L6-v2 text-embedding-3-small (OpenAI; optional) For Colab: sentence-transformers is easiest.","title":"C. An embedding model"},{"location":"stage7_colab_prereqs/#d-a-vector-store","text":"Simplest/local options: FAISS (best for Colab) Chroma (optional) FAISS keeps everything in RAM \u2192 zero infra complexity.","title":"D. A vector store"},{"location":"stage7_colab_prereqs/#e-a-retriever","text":"FAISSStore.asRetriever(k=3) or Chroma.asRetriever().","title":"E. A retriever"},{"location":"stage7_colab_prereqs/#f-an-llm","text":"At minimal: GPT-4o-mini or GPT-4.1 via API Or a local model like Qwen2.5 0.5B via transformers (only for demonstration) For Colab, simplest working pipeline uses OpenAI API.","title":"F. An LLM"},{"location":"stage7_colab_prereqs/#2-minimal-technical-requirements-software","text":"Install only these: pip install langchain sentence-transformers faiss-cpu openai Optional: PyPDF2 for PDFs tiktoken for token counting","title":"2. Minimal Technical Requirements (Software)"},{"location":"stage7_colab_prereqs/#3-minimum-pipeline-executable-sequence","text":"","title":"3. Minimum Pipeline (Executable Sequence)"},{"location":"stage7_colab_prereqs/#step-1-load-documents","text":"Read .txt or extract text from PDF","title":"Step 1. Load documents"},{"location":"stage7_colab_prereqs/#step-2-chunk-the-text","text":"Use LangChain\u2019s RecursiveCharacterTextSplitter or your own","title":"Step 2. Chunk the text"},{"location":"stage7_colab_prereqs/#step-3-embed-chunks","text":"Using Sentence Transformers model Produce vectors","title":"Step 3. Embed chunks"},{"location":"stage7_colab_prereqs/#step-4-build-index","text":"Store vectors in FAISS Save chunk texts as metadata","title":"Step 4. Build index"},{"location":"stage7_colab_prereqs/#step-5-build-retriever","text":"FAISS \u2192 retriever","title":"Step 5. Build retriever"},{"location":"stage7_colab_prereqs/#step-6-build-rag-chain","text":"Query \u2192 embed \u2192 retrieve top-K chunks \u2192 format prompt \u2192 LLM \u2192 answer","title":"Step 6. Build RAG chain"},{"location":"stage7_colab_prereqs/#step-7-test","text":"Ask a question and inspect: Retrieved chunks Generated answer","title":"Step 7. Test"},{"location":"stage7_colab_prereqs/#4-minimal-working-rag-code-skeleton-pseudocode","text":"This is not full code; it\u2019s the conceptual \u201cshape\u201d of the notebook. # 1. Load text text = open('notes.txt').read() # 2. Chunk chunks = chunk(text, size=500, overlap=100) # 3. Embed embeddings = embed(chunks) # 4. Index faiss_index = FAISS.from_embeddings(embeddings, chunks) # 5. Retriever retriever = faiss_index.as_retriever(k=5) # 6. Query query = \"What does the document say about X?\" query_vector = embed(query) results = retriever.get_relevant_documents(query) # 7. Prompt + LLM answer = call_llm(query, results) print(answer) This is literally all needed for the smallest RAG demo.","title":"4. Minimal Working RAG Code Skeleton (Pseudocode)"},{"location":"stage7_colab_prereqs/#5-minimal-notebook-structure-810-cells","text":"Install packages Import libraries Load data Chunk data Load embedding model Generate + store embeddings Build FAISS index Query via retriever Build prompt Call LLM","title":"5. Minimal Notebook Structure (8\u201310 cells)"},{"location":"stage7_colab_prereqs/#6-practical-tips-for-the-beginner-prototype","text":"Use very small datasets (2\u20133 paragraphs) so you can inspect every chunk. Print retrieved chunks each time; this is essential for learning retrieval behavior. Start with top_k = 3 . Keep your LLM prompt extremely simple in Stage 1. Do not add rerankers, hybrid search, or agent pipelines initially. Goal: functional correctness, not sophistication .","title":"6. Practical Tips for the Beginner Prototype"},{"location":"stage7_colab_prereqs/#7-smallest-buildable-rag-architecture-diagram","text":"Documents \u2500\u2192 Chunker \u2500\u2192 Embedder \u2500\u2192 FAISS Index \u2502 Query \u2500\u2192\u2502 \u25bc Retriever \u2502 \u25bc Prompt Constructor \u2502 \u25bc LLM \u25bc Answer Stage 7 complete. Say \u201ccontinue\u201d to proceed to Stage 8 \u2192 Final ultra-short summary + learning checklist .","title":"7. Smallest Buildable RAG Architecture Diagram"},{"location":"stage8_summary/","text":"Stage 8 \u2013 Final Ultra-Short Summary + Learning Checklist Ultra-short summary RAG = \u201cretrieve, then generate\u201d Retrieval quality dominates outcome Core components: embeddings, vector DB, retriever, LLM Debug by checking chunks first Build minimal Colab first Learning checklist Foundation Embeddings Chunking ANN basics Grounding Core mechanics Index building Retrieval Prompt assembly Quality tuning Hybrid retrieval Rerankers Chunk tuning Practical Build FAISS RAG Inspect retrieval Tight grounding prompts","title":"8 Final summary & checklist"},{"location":"stage8_summary/#stage-8-final-ultra-short-summary-learning-checklist","text":"","title":"Stage 8 \u2013 Final Ultra-Short Summary + Learning Checklist"},{"location":"stage8_summary/#ultra-short-summary","text":"RAG = \u201cretrieve, then generate\u201d Retrieval quality dominates outcome Core components: embeddings, vector DB, retriever, LLM Debug by checking chunks first Build minimal Colab first","title":"Ultra-short summary"},{"location":"stage8_summary/#learning-checklist","text":"","title":"Learning checklist"},{"location":"stage8_summary/#foundation","text":"Embeddings Chunking ANN basics Grounding","title":"Foundation"},{"location":"stage8_summary/#core-mechanics","text":"Index building Retrieval Prompt assembly","title":"Core mechanics"},{"location":"stage8_summary/#quality-tuning","text":"Hybrid retrieval Rerankers Chunk tuning","title":"Quality tuning"},{"location":"stage8_summary/#practical","text":"Build FAISS RAG Inspect retrieval Tight grounding prompts","title":"Practical"}]}